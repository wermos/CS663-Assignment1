\documentclass[a4paper]{article}

\usepackage[DIV=12]{typearea}
\usepackage{microtype}
\usepackage[shortlabels]{enumitem}
\usepackage{mathtools, amssymb,nccmath}
\usepackage{bm}
\setlength{\parindent}{0em}
\title{5}
\date{}

\begin{document}
\maketitle
\begin{enumerate}[(a)]
\item 
Covariance matrix in PCA is given as follows
\begin{equation}
	\bm{C} = \frac{1}{N-1}\bm{X}\bm{X}^T \text{ where $\bm{X} = \begin{bmatrix} \overline{\bm{x}}_1 & \cdots & \overline{\bm{x}}_N \end{bmatrix}$ and $\overline{\bm{x}}\in \mathbb{R}^d$}
\end{equation}
Now, this covariance matrix is symmetric as
\begin{equation}
\begin{aligned}
	\bm{C}^T &= \left(\frac{1}{N-1}\bm{X}\bm{X}^T\right)^T\\
	& = \frac{1}{N-1}(\bm{X}^T)^T \bm{X}^T \\	
	& = \frac{1}{N-1}\bm{X}\bm{X}^T \\	
	& = \bm{C} \\	
\end{aligned}
\end{equation}
Now, to prove that $\bm{C}$ is positive semi-definite we assume the opposite and show its infeasibility. That is, assume it is not positive semi-definite then there exists a vector $\bm{y}\in\mathbb{R}^d$ such that $\bm{y}^T\bm{C}\bm{y}<0$.
\begin{equation}
 	\begin{aligned}
 		\text{Now, }\exists \bm{y}\in\mathbb{R}^d | \bm{y}^T\bm{C}\bm{y}<0&\Leftrightarrow\bm{y}^T\frac{1}{N-1}\bm{X}\bm{X}^T\bm{y}<0\\
 		&\Leftrightarrow \frac{1}{N-1}\bm{y}^T\bm{X}\bm{X}^T\bm{y}<0\\
 		&\Leftrightarrow \frac{1}{N-1}(\bm{X}^T\bm{y})^T\bm{X}^T\bm{y}<0\\
 		&\Leftrightarrow \frac{1}{N-1}\|\bm{X}^T\bm{y}\|<0\\
 		&\Leftrightarrow \|\bm{X}^T\bm{y}\|<0\\
 		&\Leftrightarrow \bot
 	\end{aligned}
\end{equation}
a contradiction.
\item 
Let $\bm{v}_1, \bm{v}_2$ be two eigenvectors of $\bm{A}$ with distinct eigenvalues $\lambda_1, \lambda_2$ respectively.
Now, consider
\begin{equation}
	\begin{aligned}
		\lambda_1 \bm{v}_1^T \bm{v}_2
		&= \bm{A} \bm{v}_1^T \bm{v}_2\\
		&= \bm{A} \bm{v}_2^T \bm{v}_1 & \hfill{(\text{$\bm{v}_1^T \bm{v}_2$ is a scalar, so it is equal to its transpose})}\\
		&= \lambda_2 \bm{v}_2^T \bm{v}_1\\
		&= \lambda_2 \bm{v}_1^T \bm{v}_2 & \hfill{(\text{taking transpose again})}\\
	\end{aligned}
\end{equation}
This implies
\begin{equation}
	\begin{aligned}
		&\lambda_1 \bm{v}_1^T \bm{v}_2 - \lambda_2 \bm{v}_1^T \bm{v}_2 = 0\\
		&\Leftrightarrow(\lambda_1 - \lambda_2) \bm{v}_1^T \bm{v}_2 = 0\\
		&\Leftrightarrow\bm{v}_1^T \bm{v}_2 = 0 & \hfill{(\text{since eigenvalues are distinct})}
	\end{aligned}
\end{equation}
Hence, the eigenvectors of a symmetric matrix are orthonormal.
\item $\bm{\tilde{x}}_i=\bm{\overline{x}}+\sum_{l=1}^{k}\bm{V}_l\alpha_{il}$.

Consider, 
\begin{equation}
	\begin{aligned}
		\frac{1}{N}\sum_{i=1}^{N}\|\bm{\tilde{x}}_i-\bm{x}_i\|_2^2
		&= \frac{1}{N}\sum_{i=1}^{N}\|\underbrace{\bm{\overline{x}}+\sum_{l=1}^{k}\bm{V}_l\alpha_{il}}_{\bm{\tilde{x}}_i}-\underbrace{(\bm{\overline{x}}+\bm{V}\alpha_i)}_{\bm{x}_i}\|_2^2\\
		&= \frac{1}{N}\sum_{i=1}^{N}\|\underbrace{\bm{\overline{x}}+\sum_{l=1}^{k}\bm{V}_l\alpha_{il}}_{\bm{\tilde{x}}_i}-\underbrace{(\bm{\overline{x}}+\sum_{l=1}^{N}\bm{V}_l\alpha_{il})}_{\bm{x}_i}\|_2^2\\
		&= \frac{1}{N}\sum_{i=1}^{N}\left\|{-\sum_{l=k+1}^{N}\bm{V}_l\alpha_{il}}\right\|_2^2\\
		&= \frac{1}{N}\sum_{i=1}^{N}\left\|{\sum_{l=k+1}^{N}\bm{V}_l\alpha_{il}}\right\|_2^2\\
		&= \frac{1}{N}\sum_{i=1}^{N}\left(\left({\sum_{l=k+1}^{N}\bm{V}_l\alpha_{il}}\right)^T\left({\sum_{l=k+1}^{N}\bm{V}_l\alpha_{il}}\right)\right) & \hfill{(\|\bm{x}\|_2^2=\bm{x}^T\bm{x})}\\
		&= \frac{1}{N}\sum_{i=1}^{N}\left({\sum_{l=k+1}^{N}\bm{V}_l^T\bm{V}_l\alpha_{il}^2}\right) & \hfill{(\bm{V}_i^T\bm{V}_j = 0 \text{ for } i\neq j)}\\
		&= \frac{1}{N}\sum_{i=1}^{N}{\sum_{l=k+1}^{N}\alpha_{il}^2} & \hfill{(\bm{V}_i^T\bm{V}_i = 1)}\\
	\end{aligned}
\end{equation}
As $\alpha_{il}$ are small for $l>k$, $\mfrac{1}{N}\sum_{i=1}^{N}\|\bm{\tilde{x}}_i-\bm{x}_i\|_2^2$ is also small.

We know that $\bm{C} =\mfrac{1}{N-1}\bm{X}\bm{X}^T$, and $\bm{X}=\bm{V}\bm{S}\bm{U}^T$ which implies $\bm{C}=\mfrac{1}{N-1}\bm{V}\bm{S}\bm{S}^T\bm{V}^T$.

So, the eigenvalues $\lambda_i$ for $\bm{C}$ is $\mfrac{1}{N-1}$ times the respective singular values of $\bm{X}$.

Using this we can derive the following
\begin{equation}
	\frac{1}{N}\sum_{i=1}^{N}\|\bm{\tilde{x}}_i-\bm{x}_i\|_2^2 = \frac{N}{N-1}\sum_{i=k+1}^{N}\lambda_i\approx\sum_{i=k+1}^{N}\lambda_i
\end{equation}
Again, as these eigenvalues are small, the overall error is also small.
% We know that,
% \begin{equation}
% 	\begin{aligned}
% 		\bm{x}_i &= \bm{\overline{x}}+\alpha_{il}\bm{V}_l\\
% 		\bm{x}_i -\bm{\overline{x}} &= +\alpha_{il}\bm{V}_l\\
% 		\bm{V}_l^T(\bm{x}_i -\bm{\overline{x}}) &= \alpha_{il}\\
% 		(\bm{x}_i^T -\bm{\overline{x}}^T)\bm{V}_l &= \alpha_{il}\\
% 	\end{aligned}
% \end{equation}
\item 
Let $\bm{X}=\begin{bmatrix}X_1 & X_2\end{bmatrix}^T$. Now, $\bm{C}$ is a $2\times2$ covariance matrix with $\bm{C}_{i,j}=\operatorname{cov}(X_i, X_j)$

Since, $X_1, X_2$ are uncorrelated, $\operatorname{cov}(X_i, X_j)=0$ when $i\neq j$ and $\operatorname{cov}(X_i, X_i)=\operatorname{variance}(X_i)$.

Hence $\bm{C_1}=\begin{bmatrix}100 & 0\\0&1\end{bmatrix}$ and $\bm{C_2}=\begin{bmatrix}1 & 0\\0&1\end{bmatrix}$.

The principal components are the eigenvectors of $\bm{C}$. In this context, it means set of uncorrelated vectors that can be used to express any $\bm{X}$.

Since, $X_1, X_2$ are already given as uncorrelated, the principal components for both cases are the standard unit vectors $e_1, e_2$ which are also the eigenvectors of  $\bm{C}$.
\end{enumerate}
\end{document}